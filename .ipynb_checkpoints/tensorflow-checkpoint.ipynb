{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shubham/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/shubham/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/shubham/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/shubham/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/shubham/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/shubham/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/shubham/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/shubham/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/shubham/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/shubham/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/shubham/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/shubham/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "# Required Downloads\n",
    "\"\"\"\n",
    "    libraries down\n",
    "    facenet_keras model : https://drive.google.com/drive/folders/12aMYASGCKvDdkygSv1yQq8ns03AStDO_\n",
    "    haarcascade opencv classifier : https://github.com/opencv/opencv/blob/master/data/haarcascades/haarcascade_frontalface_default.xml\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define facedetector and model , both takes time\n",
    "model=tf.keras.models.load_model('facenet_keras.h5',compile=False)\n",
    "facedetector=cv2.CascadeClassifier(\"haarcascade_frontalface_default.xml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_student(directory,name):\n",
    "    try:\n",
    "        os.mkdir(os.path.join(directory,name))\n",
    "    except OSError as error:\n",
    "        print('Student already exists')\n",
    "        return \n",
    "        \n",
    "    cam=cv2.VideoCapture(0)\n",
    "    pic_count=-2\n",
    "    while True :\n",
    "        ret,frame=cam.read()\n",
    "        if ret==0:\n",
    "            break \n",
    "        faces=facedetector.detectMultiScale(\n",
    "            image=frame,\n",
    "            scaleFactor=1.1,\n",
    "            minNeighbors=8\n",
    "        )\n",
    "        for (x,y,w,h) in faces:\n",
    "            pic_count+=1\n",
    "            if pic_count>0:\n",
    "                cv2.imwrite(os.path.join(os.path.join(directory,name),str(pic_count)+'.jpg'),frame[y:y+h,x:x+w])\n",
    "            cv2.rectangle(frame,(x,y),(x+w,y+h),(255,0,0),1)\n",
    "        cv2.imshow('img',frame)\n",
    "        if cv2.waitKey(10)&0xff==ord('q'):\n",
    "            break\n",
    "    cam.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_students_encoding(model,directory):\n",
    "    students_encoding={}\n",
    "    list_of_students=os.listdir(directory)\n",
    "    for student in list_of_students:\n",
    "        images=[]\n",
    "        newdirectory=os.path.join(directory,student)\n",
    "        for pic in os.listdir(newdirectory):\n",
    "            image=cv2.imread(os.path.join(newdirectory,pic))\n",
    "            images.append(cv2.resize(image,(160,160),cv2.INTER_AREA)/255.0)\n",
    "       \n",
    "        if len(images)==0:\n",
    "            break\n",
    "            \n",
    "        dataset=np.array(images)\n",
    "        encode=model.predict(dataset)\n",
    "        students_encoding[student]=encode\n",
    "    \n",
    "    return students_encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculatedistance(encode1,encode2):\n",
    "    distance=tf.reduce_sum(tf.square(tf.subtract(encode1,encode2)))\n",
    "    return distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_min_distance(test_encoding,list_of_encodings):\n",
    "    dist=1000\n",
    "    for encoding in list_of_encodings:\n",
    "        dist=min(dist,np.linalg.norm(test_encoding-encoding))\n",
    "        #dist=min(dist,calculatedistance(test_encoding,encoding))\n",
    "    return dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_student(test_encoding,students_encoding,threshold):\n",
    "    dist=10000000000000\n",
    "    id=\"unknown\"\n",
    "    for name,encodings in students_encoding.items():\n",
    "        newdist=get_min_distance(test_encoding,encodings)\n",
    "        print(newdist,name)\n",
    "        if newdist<dist:\n",
    "            dist=newdist\n",
    "            id=name\n",
    "    print(dist,id)\n",
    "    if dist>threshold:\n",
    "        id=\"\"\n",
    "    return id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory=r'facedatabase'\n",
    "threshold=8.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import face_recognition as fc\n",
    "def take_attendance(directory,threshold,model):\n",
    "    # Face recognition (with liveness detection) code\n",
    "    # Generate and store the face encodings of known students using their face images.\n",
    "    # To be done using pre trained model( FaceNet) but here i used face_recognition python library .\n",
    "    # Images of students are stored in 'students_images' directory .\n",
    "   \n",
    "    known_faces_encodings=get_students_encoding(model,directory)\n",
    "    video=cv2.VideoCapture(0)\n",
    "\n",
    "    success=1\n",
    "\n",
    "    # OpenCV Haar-feature based cascade classifier for detecting eyes \n",
    "    eye_cascade=cv2.CascadeClassifier('haarcascade_eye.xml')\n",
    "\n",
    "    # load the model trained for predicting whether or not the eyes are closed\n",
    "    json_file=open(\"eye_model.json\",\"r\")\n",
    "    eye_model=tf.keras.models.model_from_json(json_file.read())\n",
    "    json_file.close()\n",
    "    eye_model.load_weights(\"eye_model.h5\")\n",
    "\n",
    "\n",
    "    # Dictionary with key representing the rollno of recognised student and value representing the status of eyes ( either open or closed )\n",
    "    recognised_students={}\n",
    "\n",
    "    while success:\n",
    "\n",
    "        sucess,frame=video.read()\n",
    "        frame_gray=cv2.cvtColor(frame,cv2.COLOR_BGR2GRAY)\n",
    "        is_processed=0\n",
    "        if is_processed==0 :\n",
    "\n",
    "\n",
    "            detected_faces_locations = fc.face_locations(frame)\n",
    "            detected_faces_encodings = fc.face_encodings(frame,detected_faces_locations)\n",
    "\n",
    "            detected_faces_rollno=[]\n",
    "\n",
    "            # For each detected face in the frame generate its face encoding and compare it with encodings of known students.\n",
    "            # If matched append its rollno in an array or append 'unknown'\n",
    "            for encoding in detected_faces_encodings:\n",
    "                rollno=identify_student(encoding,known_faces_encodings,100000)\n",
    "                detected_faces_rollno.append(rollno)\n",
    "        \n",
    "                \n",
    "\n",
    "            # For each detected Face do \n",
    "            for (top,right,bottom,left),rollno in zip(detected_faces_locations,detected_faces_rollno):\n",
    "                \n",
    "               \n",
    "                if rollno=='unknown':\n",
    "                    # if person is unknown do noting\n",
    "                    cv2.rectangle(frame,(left,top),(right,bottom),(0,0,255),2)\n",
    "                else:\n",
    "                    # if student is known do liveness detection using blink detection\n",
    "                    cv2.rectangle(frame,(left,top),(right,bottom),(0,255,0),2)\n",
    "\n",
    "                    # detect eyes in the face\n",
    "                    roi_gray = frame_gray[top:bottom,left:right]\n",
    "                    eyes= eye_cascade.detectMultiScale(roi_gray)\n",
    "\n",
    "                    # If any one or both the eyes are open flag becomes 1\n",
    "                    flag=0\n",
    "                    for (ex,ey,ew,eh) in eyes:\n",
    "                        roi=frame[ey+top:ey+eh+top,ex+left:ex+left+ew]\n",
    "                        roi=cv2.resize(roi,(32,32),interpolation=cv2.INTER_AREA)\n",
    "                        roi=np.expand_dims(roi,axis=0)\n",
    "                        roi=np.reshape(roi,(1,32,32,3))\n",
    "                        image_to_classify=np.vstack([roi])\n",
    "\n",
    "                        classes=eye_model.predict(image_to_classify,batch_size=10)\n",
    "\n",
    "                        # If probab that eye is open is greater than 0.5 predict eye is open\n",
    "                        if classes[0][0]>=0.5 :\n",
    "                            flag=1\n",
    "                        cv2.rectangle(frame,(ex+left,ey+top),(ex+ew+left,ey+eh+top),(0,255,0),2)\n",
    "\n",
    "                    status='Closed'\n",
    "                    if flag==1:\n",
    "                        status='Open'\n",
    "\n",
    "                    cv2.putText(frame,rollno,(left+6,bottom-6),cv2.FONT_HERSHEY_DUPLEX,1.0,(255,255,255),1)\n",
    "                    cv2.putText(frame,status,(left+6,bottom+20),cv2.FONT_HERSHEY_DUPLEX,1.0,(255,0,0),1)\n",
    "\n",
    "                    # If eyes are closed now but previously it was observed to be open->Blink detected\n",
    "                    if status=='Closed':\n",
    "                        if rollno not in recognised_students:\n",
    "                            recognised_students.update({rollno:0})\n",
    "                        else:\n",
    "                            if recognised_students[rollno]==1:\n",
    "                                #---------------------------------------\n",
    "                                print('Present '+rollno+' '+str(time.ctime()))\n",
    "                                #---------------------------------------\n",
    "                            recognised_students[rollno]=0\n",
    "\n",
    "                    else:\n",
    "                        if rollno not in recognised_students:\n",
    "                            recognised_students.update({rollno:1})\n",
    "                        else:\n",
    "                            recognised_students[rollno]=1\n",
    "\n",
    "\n",
    "\n",
    "            cv2.imshow('Videos',frame)\n",
    "            \n",
    "            if cv2.waitKey(1)&0xff==ord('q'):\n",
    "                break\n",
    "            success=1\n",
    "            is_processed=1\n",
    "\n",
    "    cv2.destroyAllWindows()\n",
    "    video.release()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_student(directory,\"2017UCO1681\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'test_encodings' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-62-b98fd5f4ed0d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtake_attendance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mthreshold\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-58-e698e4916731>\u001b[0m in \u001b[0;36mtake_attendance\u001b[0;34m(directory, threshold, model)\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0;31m# If matched append its rollno in an array or append 'unknown'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mencoding\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdetected_faces_encodings\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m                 \u001b[0mrollno\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0midentify_student\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mknown_faces_encodings\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m100000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m                 \u001b[0mdetected_faces_rollno\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrollno\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-56-8da5cfbae2d2>\u001b[0m in \u001b[0;36midentify_student\u001b[0;34m(test_encoding, students_encoding, threshold)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstudents_encoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mencodings\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstudents_encoding\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencodings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_encodings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0mnewdist\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mget_min_distance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_encoding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mencodings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnewdist\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test_encodings' is not defined"
     ]
    }
   ],
   "source": [
    "take_attendance(directory,threshold,model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'video' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-d107a4851b24>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mvideo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'video' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
